---
title: "02_stationary_models_stan"
author: "Steffen Ernst"
date: "2024-06-14"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (!require("pacman")) install.packages("pacman"); library(pacman) #installs and loads package handler
p_load(tidyverse, janitor, forcats, bayesplot, RcppParallel, rstan, BayesFactor, bridgesampling, loo, reticulate, patchwork) #installs/loads all necessary packages

setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #sets Path to where this file is stored
options(mc.cores = parallel::detectCores())

```

The goal of this script is to generate (stationary) models using STAN that encompass a drift-diffusion Model for binary decisions. The data used was obtained in an experiment that I ran as part of my master thesis. In it the participants had to decide whether a picture has a majority of blue or orange pixels.

The experiment has 2 conditions, speed and accuracy, that switch every 48 tries. At the same time the difficulty changes every 8 or 16 tries between 4 levels.
Model1 has no interaction term between the difficulty and speed/accuracy, Model2 has an interaction term. In practice that means that Model2 encompasses 8 values for v based on the difficulty and the speed/accuracy condition and Model1 only 4 values based on the difficulty.

After creation of the models, leave-one-out-cross validation is used to check whether the more complicated model outperforms the basic model.


```{r download files}
dir.create(path = "STAN") #create folder where models are stored.
df <-  read_csv("https://raw.githubusercontent.com/steffenernst96/master-thesis/main/data_cleaned/df_min.csv")
download.file(url = "https://raw.githubusercontent.com/steffenernst96/master-thesis/main/STAN/model1.stan",
              destfile = "STAN\\model1.stan")
download.file(url = "https://raw.githubusercontent.com/steffenernst96/master-thesis/main/STAN/model2.stan",
              destfile = "STAN\\model2.stan")
```
The following function is used to simulate reaction times (rt) and correct(y/n) from the input variables. We use it here for posterior retrodiction checks of the models.

```{r wiener-process function}
wiener_process <- function (v=1, a=1, z=0.5, ndt=0.3) {
  # standard DDM model parameters
  # v   = drift rate
  # a   = boundary separation
  # z   = starting point; z = 0.5 corresponds to no bias
  # ndt = non-decision time in s
  
  max_iter <- 1e4 # maximum process duration in ms
  dt <- 0.001    # time steps
  sd <- 1        # sd for noise
  sqrt_dt <- sqrt(dt*sd)
  
  # initialize diffusion path for current trial
  path <-  a * z
  # sample diffusion process noise
  noise <- rnorm(max_iter, 0, 1)
  
  # evidence accumulation process
  iter <- 1
  while (path > 0 & path < a & iter < max_iter) {
    path <- path + v*dt + sqrt_dt*noise[iter]
    iter <- iter + 1
  }
  
  # return response time [s] and choice [0, 1]
  return(c(ndt + iter*dt, as.numeric(path > a)))
}
```


```{r Initialization function}
# initialisation function to prevent starting values of MCMC chain outside of possible values for ndt.

init = function(chains=4, min_rt) {
  L = list()
  for (c in 1:chains) {
    L[[c]]=list()
    L[[c]]$ndt = runif(1, 0.1, min_rt*0.95) # because ndt is always smaller than the fastest reaction time it has to be below min_rt
   
  }
  return (L)
}

```

This chunk needs a long time to run: it creates 2 Models per person for all 14 participants.
2 models * 14 participants  * 4 chains * (4000) chainlength

```{r create STAN Models ALL participants}
nr_of_chains = 4 #number of chains
n_warmup = 2000 # number of warmup iterations per chain
n_iter = 4000
ldf2 = list()
for (i in unique(df$id)) {
  df_temp <- df %>% filter(id==i,
                           tutorial==0)
  border <- boxplot.stats(log(df_temp$rt))$stats[1] # outlier elimination corresponding to: lower quartile - 1.5*IQR (interquartilerange). We only want to eliminate lower outliers because model generation doesn't handle them well. ((non-decision-time(ndt) gets unrealistic with low outliers)
  print(c("subject_nr=", i, table(log(df_temp$rt) > border))) # FALSE is amount of outliers
  df_temp <- df_temp %>% filter(log(df_temp$rt) > border)
  

  rt <- df_temp$rt
  min_rt <- min(rt)
  correct <- as.numeric(as.character(df_temp$correct))
  difficulty <- df_temp$difficulty
  condition <- df_temp$speed_condition
  condition2 <- ifelse(df_temp$speed_condition==1, 4,0) # model2 has an interaction term, so we need 8 instead of 4 values to be calculated for v
  N <- nrow(df_temp)
  
  data_list <- list(N = N,
                    rt = rt,
                    correct = correct,
                    difficulty = difficulty,
                    condition = condition,
                    condition2 = condition2)
  model_file <- "STAN\\model1.stan"
  fit1 <- stan(
    file = model_file,
    data = data_list,    # named list of data
    chains = nr_of_chains,             # number of Markov chains
    warmup = n_warmup,          # number of warm-up iterations per chain
    iter = n_iter,#,            # total number of iterations per chain
    init = init(nr_of_chains, min_rt),   # initialization of chain for ndt lower than min_rt, see function 
    cores = parallel::detectCores() # number of cores
    # refresh = 0             # no progress shown
  )
  saveRDS(fit1, file = paste0("STAN\\subject_",df_temp[1,"id"] ,"_fit1.rds"))

  model_file <- "STAN\\model2.stan"
  fit2 <- stan(
    file = model_file,
    data = data_list,    # named list of data
    chains = nr_of_chains,  # number of Markov chains
    warmup = n_warmup,          # number of warm-up iterations per chain
    iter = n_iter,            # total number of iterations per chain
#    init = init(nr_of_chains, min_rt),   # initialization of chain for ndt lower than min_rt, see function 
    cores = parallel::detectCores()              # number of cores (could use one per chain)

    # refresh = 0             # no progress shown
  )
  saveRDS(fit2, file = paste0("STAN\\subject_",df_temp[1,"id"] ,"_fit2.rds"))
  listname = paste0("fitlist_", df_temp[1,"id"])
  #list_temp <- list(fit1 = fit1, fit2 = fit2)
  ldf2[[paste0("subject_",df_temp[1,"id"] ,"_fit1.rds")]] <- fit1
  ldf2[[paste0("subject_",df_temp[1,"id"] ,"_fit2.rds")]] <- fit2
  
}


```

This chunk of code loads already calculated models from my private env. If the above code doesn't work in your env but you need the models, don't hesitate to contact me at steffen.e1996@gmail.com.
If you ran the code and ldf2 is created, then don't run this chunk.

```{r load all STAN Models, eval = FALSE}
filenames <- list.files("C:/Users/steff/Documents/Universität/Master Psychologie/Masterarbeit/STAN", pattern="*.rds", full.names=TRUE)
ldf <- lapply(filenames, read_rds)

filenames_shortened <-str_sub(filenames, start = -18, end = -5)
filenames_shortened <- gsub("^(?!s)", "s", filenames_shortened, perl = T)
for (i in 1:length(ldf)) {
  ldf[[i]]@model_name <- filenames_shortened[i]
}
```


This chunk also runs for quite some time.
Values for loo's (leave-one-out-cross-validation) are created. 
```{r  create loos}
loo_list <-  list()
ldf_id <- c(15, 1, 
            24, 10, 
            25, 11, 
            26, 12, 
            27, 13, 
            28, 14, 
            16, 2, 
            17, 3, 
            18, 4, 
            19, 5, 
            20, 6, 
            21, 7, 
            22, 8, 
            23, 9)
position <- match(15:28, ldf_id) # use the match function to find positions
if ("ldf2" %in% ls()) { # if ldf2 is already created, we use that as the list 
  ldf <- ldf2
  rm(ldf2)
}

#this loop calculates for a while!
for (i in 1:length(ldf)) { #does leave-one-out cross validation
  loo_list[[i]] <- loo(ldf[[i]])
}
names(loo_list) <-  filenames_shortened


#create a list with comparisons for all subjects
loocomparison_list <-  list()
j=1
for (i in 1:(length(ldf)/2)) {
  loocomparison_list[[i]] <-  loo_compare(loo_list[[j]],loo_list[[j+1]])
  j=j+2

}
#results = c(2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2)

reorder_rows <- function(df) {
  # Check if 'model1' is in the row names
  if ("model1" %in% rownames(df)) {
    # Move 'model1' to the first row
    df <- df[c("model1", setdiff(rownames(df), "model1")), ]
  }
  return(df)
}

# Apply the reorder_rows function to each data frame in the list
reordered_loocomparison_list <- lapply(loocomparison_list, reorder_rows)
combined_df <- as_tibble(do.call(rbind, reordered_loocomparison_list))
combined_df <- combined_df %>% 
  select("elpd_loo", "se_elpd_loo", "elpd_diff", "se_diff") %>% 
  add_column(Modell =rep(c("M1","M2"), 14), .before = "elpd_loo") %>%
  add_row(Modell = "Versuchsperson 1",.before = 1) %>% 
  add_row(Modell = "Versuchsperson 2",.before = 4) %>% 
  add_row(Modell = "Versuchsperson 3",.before = 7) %>% 
  add_row(Modell = "Versuchsperson 4",.before = 10) %>% 
  add_row(Modell = "Versuchsperson 5",.before = 13) %>% 
  add_row(Modell = "Versuchsperson 6",.before = 16) %>% 
  add_row(Modell = "Versuchsperson 7",.before = 19) %>% 
  add_row(Modell = "Versuchsperson 8",.before = 22) %>% 
  add_row(Modell = "Versuchsperson 9",.before = 25) %>% 
  add_row(Modell = "Versuchsperson 10",.before = 28) %>% 
  add_row(Modell = "Versuchsperson 11",.before = 31) %>% 
  add_row(Modell = "Versuchsperson 12",.before = 34) %>% 
  add_row(Modell = "Versuchsperson 13",.before = 37) %>% 
  add_row(Modell = "Versuchsperson 14",.before = 40)

kable_table <- kable(combined_df, format = "html", digits=2)
kable_table

```

The  ELPD (expected log predictive density, Vehtari et al., 2017) was used as a benchmark for model comparison. The lower the ELPD, the better the model fits to the data. We can see that for 9 out of 14 participants the more complex model2 was better. For 5 out of 14 participants, the difference was > 4, a somewhat big difference (Sivula et al., 2020). But we also have to consider the uncertainty in calculating these measures, expressed via the standard error of ELPD. In all 5 cases ELPD < 2*ELPD-SE indicating high uncertainty. For 5 out of 14 participants M1 was better but always with ELPD<4. In conclusion we can say that M2 is marginally better but because of the increased complexity M1 is to be preferred.

Thus in Script Nr.3 we use our Model1 as a comparison for our dynamic Model. 


____________________________________
Vehtari, A., Gelman, A. & Gabry, J. (2016). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4
Sivula, T. (2020, 24. August). Uncertainty in Bayesian Leave-One-Out Cross-Validation based model comparison. arXiv.org. https://arxiv.org/abs/2008.10296




The following code is legacy code that was not used directly in my thesis. So this part of the script is not that well maintained.
```{r code to create a single model, eval = FALSE}
df_temp <- df %>% 
  filter(tutorial == 0,
         id == 5)
  
rt <- df_temp$rt
correct <- df_temp$correct
difficulty <- df_temp$difficulty
condition <- df_temp$speed_condition
condition2 <- ifelse(df_temp$speed_condition==1, 4,0)
N <- nrow(df_temp)

  # Create data list
data_list <- list(N = N,
                    rt = rt,
                    correct = correct,
                    difficulty = difficulty,
                    condition = condition,
                    condition2 = condition2)

model_file <- "STAN\\model1.stan"
set.seed(12345)
fit1 <- stan(
    file = model_file,
    data = data_list,    # name d list of data
    chains = 4,             # number of Markov chains
    warmup = 2000,          # number of warmup iterations per chain
    iter = 4000,#,            # total number of iterations per chain
    cores = parallel::detectCores(),              # number of cores (could use one per chain)
    init(nr_of_chains, min_rt)
    # refresh = 0             # no progress shown
  )

saveRDS(fit1, "STAN/fit1.rds")

model_file <- "STAN\\model2.stan"
fit2 <- stan(
    file = model_file,
    data = data_list,    # name d list of data
    chains = 4,             # number of Markov chains
    warmup = 2000,          # number of warmup iterations per chain
    iter = 4000,#,            # total number of iterations per chain
    cores = parallel::detectCores()   # number of cores (could use one per chain)
    #init(nr_of_chains, min_rt)
    # refresh = 0             # no progress shown
  )

saveRDS(fit2, "STAN/fit2.rds")
```



```{r load single model, include = FALSE, eval= FALSE}
fit1 <- readRDS("STAN\\subject_1_fit1.rds")
fit2 <- readRDS("STAN\\subject_1_fit2.rds")
```

```{r model1 checks, eval= FALSE}
# Check convergence
print(fit1, par=c("a", "v", "ndt"), digits=2)
print(rstan::stan_rhat(fit1))
#mcmc_trace(fit1)#, pars = "ndt")
stan_trace(fit1)
mcmc_pairs(fit1, pars = c("v[1]", "v[2]", "v[3]", "v[4]", "a[1]"))
mcmc_pairs(fit1, pars = c("a[1]", "a[2]", "ndt"))
mcmc_pairs(fit1, pars = c("v[1]", "v[2]", "v[3]", "v[4]", "a[1]", "a[2]", "ndt"))
  
```

```{r model2 checks, eval= FALSE}
# Check convergence
print(fit2, par=c("a", "v", "ndt"), digits=2)
print(rstan::stan_rhat(fit2))
#mcmc_trace(fit1)#, pars = "ndt")
stan_trace(fit2, pars = c("v[1]", "v[2]", "v[3]", "v[4]", "v[5]", "v[6]", "v[7]", "v[8]"))
mcmc_pairs(fit2, pars = c("v[1]", "v[2]", "v[3]", "v[4]"))
mcmc_pairs(fit2, pars = c("v[5]", "v[6]", "v[7]", "v[8]"))           
mcmc_pairs(fit2, pars = c("v[1]", "v[2]", "v[3]", "v[4]", "a[1]", "a[2]", "ndt"))
mcmc_pairs(fit2, pars = c("v[5]", "v[6]", "v[7]", "v[8]", "a[1]", "a[2]", "ndt"))
mcmc_pairs(fit2, pars = c("v[1]", "v[2]", "v[3]", "v[4]", "v[5]", "v[6]", "v[7]", "v[8]"))
mcmc_pairs(fit2, pars = c("a[1]", "a[2]", "ndt"))

```


Posterior retrodiction Tests:
```{r posterior retrodiction checks, eval= FALSE}

posterior_samples <- rstan::extract(fit1, pars = c("v[1]", "v[2]", "v[3]", "v[4]", "a[1]", "a[2]", "ndt"))
j=1
sim_data <- tibble(response_time_fake = double(), correct_fake = integer())
for (i in 1:nrow(df_temp)) {
  v <- posterior_samples[[1 + difficulty[i]]] [j]
  a <- posterior_samples[[5 + condition[i]]] [j]
  ndt <- posterior_samples[[7]] [j]
  output <- wiener_process(v=v, a=a,ndt=ndt)
  sim_data <- sim_data %>% add_row(response_time_fake = output[1], correct_fake = output[2])
  j <-  ifelse(j>199,1,j+1)
  
}
df_vis <- tibble(sim_data, response_time_real = rt, correct, condition, difficulty)

ggplot(df_vis, aes(x = response_time_fake)) + #, fill = factor(correct))) +
  geom_density()+ #aes(fill = "red")) +
  geom_density(aes(x=response_time_real)) +#aplha = 0.5, fill = "blue")) + 
  facet_grid(difficulty ~ condition,labeller = labeller(difficulty = as.character, 
                               condition = function(x) ifelse(x==1, "Speed", "Accuracy"))) +
  xlim(0,2) +
  labs(title = "RT in Abhängigkeit von Bedingung und Schwierigkeit",
       x = "RT",
       y = "Density",
       fill = "Correct")



df_resp_prob <- df_vis %>%
  group_by(condition, difficulty) %>%
  summarise(resp_prob_real = mean(correct),
            resp_prob_fake = mean(correct_fake)) %>%
  pivot_longer(cols = c(resp_prob_real, resp_prob_fake),
               names_to = "response_type",
               values_to = "response_prob") %>%
  mutate(response_type = factor(response_type,
                                 levels = c("resp_prob_real", "resp_prob_fake"),
                                 labels = c("Real", "Fake")))
df_resp_prob %>%  
  filter(condition == 0) %>% #speed
  ggplot(aes(x = factor(difficulty), y = response_prob, fill = response_type)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), color = "black") +
  scale_fill_manual(values = c("Real" = "steelblue", "Fake" = "firebrick")) +
  labs(x = "Difficulty", y = "Antwortwahrscheinlichkeit", fill = "Datenart") +
  theme_classic() + 
  ggtitle("Speed-Bedingung")

df_resp_prob %>%  
  filter(condition == 1) %>% #accuracy
  ggplot(aes(x = factor(difficulty), y = response_prob, fill = response_type)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), color = "black") +
  scale_fill_manual(values = c("Real" = "steelblue", "Fake" = "firebrick")) +
  labs(x = "Difficulty", y = "Antwortwahrscheinlichkeit", fill = "Datenart") +
  theme_classic() + 
  ggtitle("Speed-Bedingung")

kable(df_resp_prob, format = "markdown", digits = 2, align = "c")
```
